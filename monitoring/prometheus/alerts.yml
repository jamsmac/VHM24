# ============================================================================
# Prometheus Alert Rules for VendHub Manager
# ============================================================================
# Comprehensive alerting for production monitoring

groups:
  # ==========================================================================
  # Backend API Alerts
  # ==========================================================================
  - name: backend_api_alerts
    interval: 30s
    rules:
      - alert: BackendAPIDown
        expr: up{job=~"vendhub-backend.*"} == 0
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Backend API is down"
          description: "Backend API has been down for more than 2 minutes"
          runbook_url: "https://docs.vendhub.com/runbooks/backend-down"

      - alert: RailwayBackendUnreachable
        expr: probe_success{job="blackbox-http", instance=~".*railway.*"} == 0
        for: 3m
        labels:
          severity: critical
          component: api
          deployment: railway
        annotations:
          summary: "Railway backend is unreachable"
          description: "Cannot reach the Railway-deployed backend at {{ $labels.instance }}"

      - alert: HighErrorRate
        expr: (sum(rate(vendhub_http_request_errors_total[5m])) / sum(rate(vendhub_http_requests_total[5m]))) > 0.05
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"

      - alert: CriticalErrorRate
        expr: (sum(rate(vendhub_http_request_errors_total[5m])) / sum(rate(vendhub_http_requests_total[5m]))) > 0.15
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Critical error rate - immediate attention required"
          description: "Error rate is {{ $value | humanizePercentage }} - above 15% threshold"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(vendhub_http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s"

      - alert: CriticalResponseTime
        expr: histogram_quantile(0.95, rate(vendhub_http_request_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Critical response time - API severely degraded"
          description: "95th percentile response time is {{ $value }}s - above 5s threshold"

      - alert: HighMemoryUsage
        expr: process_memory_bytes{type="rss"} / 1024 / 1024 > 1500
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize }}MB"

      - alert: CriticalMemoryUsage
        expr: process_memory_bytes{type="rss"} / 1024 / 1024 > 1800
        for: 5m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "Critical memory usage - approaching limit"
          description: "Memory usage is {{ $value | humanize }}MB - may cause OOM soon"

      - alert: LowRequestRate
        expr: sum(rate(vendhub_http_requests_total[5m])) < 0.1 and hour() >= 9 and hour() <= 21
        for: 15m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Unusually low request rate during business hours"
          description: "Request rate is {{ $value }}/s - check if system is accessible"

  # ==========================================================================
  # Worker Alerts
  # ==========================================================================
  - name: worker_alerts
    interval: 30s
    rules:
      - alert: WorkerDown
        expr: up{component="worker"} == 0
        for: 2m
        labels:
          severity: critical
          component: worker
        annotations:
          summary: "Worker {{ $labels.job }} is down"
          description: "Worker has been down for more than 2 minutes"

      - alert: HighJobFailureRate
        expr: (rate(bullmq_queue_failed[10m]) / (rate(bullmq_queue_completed[10m]) + rate(bullmq_queue_failed[10m]))) > 0.1
        for: 10m
        labels:
          severity: warning
          component: worker
        annotations:
          summary: "High job failure rate"
          description: "Job failure rate is {{ $value | humanizePercentage }}"

      - alert: QueueBacklog
        expr: sum(bullmq_queue_waiting) > 500
        for: 15m
        labels:
          severity: warning
          component: worker
        annotations:
          summary: "High queue backlog"
          description: "Queue has {{ $value }} waiting jobs - processing may be delayed"

      - alert: CriticalQueueBacklog
        expr: sum(bullmq_queue_waiting) > 1000
        for: 10m
        labels:
          severity: critical
          component: worker
        annotations:
          summary: "Critical queue backlog"
          description: "Queue has {{ $value }} waiting jobs - immediate investigation required"

      - alert: StalledJobs
        expr: sum(bullmq_queue_active) > 0 and increase(bullmq_queue_completed[10m]) == 0
        for: 15m
        labels:
          severity: warning
          component: worker
        annotations:
          summary: "Jobs appear to be stalled"
          description: "Active jobs present but no completions in 15 minutes"

  # ==========================================================================
  # Database Alerts
  # ==========================================================================
  - name: database_alerts
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute"

      - alert: HighDatabaseConnections
        expr: pg_stat_database_numbackends{datname="vendhub"} > 80
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "High number of database connections"
          description: "Database has {{ $value }} active connections"

      - alert: CriticalDatabaseConnections
        expr: pg_stat_database_numbackends{datname="vendhub"} > 95
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Critical database connection count"
          description: "Database has {{ $value }} connections - approaching limit"

      - alert: DatabaseDiskSpaceWarning
        expr: pg_database_size_bytes{datname="vendhub"} / (1024*1024*1024) > 30
        for: 10m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database disk space growing"
          description: "Database size is {{ $value | humanize }}GB"

      - alert: DatabaseDiskSpaceCritical
        expr: pg_database_size_bytes{datname="vendhub"} / (1024*1024*1024) > 50
        for: 10m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database disk space critical"
          description: "Database size is {{ $value | humanize }}GB - cleanup required"

      - alert: HighDatabaseRollbackRate
        expr: rate(pg_stat_database_xact_rollback{datname="vendhub"}[5m]) / (rate(pg_stat_database_xact_commit{datname="vendhub"}[5m]) + rate(pg_stat_database_xact_rollback{datname="vendhub"}[5m])) > 0.05
        for: 10m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "High database rollback rate"
          description: "Rollback rate is {{ $value | humanizePercentage }} - check for deadlocks or constraint violations"

      - alert: SlowDatabaseQueries
        expr: histogram_quantile(0.95, sum(rate(vendhub_database_query_duration_seconds_bucket[5m])) by (le)) > 1
        for: 10m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "P95 query duration is {{ $value }}s"

  # ==========================================================================
  # Redis Alerts
  # ==========================================================================
  - name: redis_alerts
    interval: 30s
    rules:
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 1 minute"

      - alert: RedisHighMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

      - alert: RedisCriticalMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) > 0.95
        for: 2m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis memory critically high"
          description: "Redis memory usage is {{ $value | humanizePercentage }} - eviction imminent"

      - alert: RedisHighLatency
        expr: redis_commands_duration_seconds_total / redis_commands_total > 0.01
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis latency is high"
          description: "Average command latency is {{ $value }}s"

      - alert: LowCacheHitRate
        expr: (sum(rate(vendhub_cache_hits_total[5m])) / (sum(rate(vendhub_cache_hits_total[5m])) + sum(rate(vendhub_cache_misses_total[5m])))) < 0.5
        for: 15m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit rate is {{ $value | humanizePercentage }} - cache may be ineffective"

  # ==========================================================================
  # Storage Alerts
  # ==========================================================================
  - name: storage_alerts
    interval: 30s
    rules:
      - alert: MinIODown
        expr: up{job="minio"} == 0
        for: 2m
        labels:
          severity: critical
          component: storage
        annotations:
          summary: "MinIO is down"
          description: "MinIO storage has been down for more than 2 minutes"

      - alert: MinIODiskSpaceWarning
        expr: ((minio_cluster_capacity_usable_total_bytes - minio_cluster_capacity_usable_free_bytes) / minio_cluster_capacity_usable_total_bytes) > 0.7
        for: 10m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "MinIO disk space growing"
          description: "MinIO disk usage is {{ $value | humanizePercentage }}"

      - alert: MinIODiskSpaceCritical
        expr: ((minio_cluster_capacity_usable_total_bytes - minio_cluster_capacity_usable_free_bytes) / minio_cluster_capacity_usable_total_bytes) > 0.9
        for: 5m
        labels:
          severity: critical
          component: storage
        annotations:
          summary: "MinIO disk space critical"
          description: "MinIO disk usage is {{ $value | humanizePercentage }}"

  # ==========================================================================
  # Infrastructure Alerts
  # ==========================================================================
  - name: infrastructure_alerts
    interval: 30s
    rules:
      - alert: HighHostCPU
        expr: 100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High host CPU usage"
          description: "Host CPU usage is {{ $value | humanize }}%"

      - alert: HighHostMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High host memory usage"
          description: "Host memory usage is {{ $value | humanize }}%"

      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 80
        for: 10m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High disk usage"
          description: "Disk usage is {{ $value | humanize }}%"

      - alert: CriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 90
        for: 5m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "Critical disk usage"
          description: "Disk usage is {{ $value | humanize }}% - immediate cleanup required"

  # ==========================================================================
  # Security Alerts
  # ==========================================================================
  - name: security_alerts
    interval: 30s
    rules:
      - alert: HighLoginFailureRate
        expr: rate(vendhub_login_failures_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High login failure rate detected"
          description: "Login failure rate is {{ $value }}/s - possible brute force attack"

      - alert: RateLimitExceeded
        expr: increase(vendhub_http_request_errors_total{status="429"}[5m]) > 100
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High rate limiting activity"
          description: "{{ $value }} requests rate limited in last 5 minutes - possible attack"

      - alert: SuspiciousLoginPattern
        expr: increase(vendhub_login_failures_total{reason="invalid_credentials"}[1h]) > 50
        for: 10m
        labels:
          severity: critical
          component: security
        annotations:
          summary: "Suspicious login pattern detected"
          description: "{{ $value }} failed logins in the last hour - possible credential stuffing attack"

      - alert: SessionAnomalies
        expr: rate(vendhub_sessions_created_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "Unusual session creation rate"
          description: "Session creation rate is {{ $value }}/s - investigate for potential abuse"

      - alert: TokenRefreshAnomalies
        expr: rate(vendhub_http_requests_total{route="/api/auth/refresh", status="401"}[5m]) > 1
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High token refresh failures"
          description: "Token refresh failure rate is {{ $value }}/s - possible token theft"

      - alert: Excessive2FAFailures
        expr: increase(vendhub_2fa_authentications_total{result="failure"}[1h]) > 20
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "Excessive 2FA failures"
          description: "{{ $value }} 2FA failures in the last hour - possible attack"

  # ==========================================================================
  # Business Alerts
  # ==========================================================================
  - name: business_alerts
    interval: 60s
    rules:
      - alert: LowTaskCompletionRate
        expr: sum(increase(vendhub_tasks_completed_total{status="completed"}[24h])) / sum(increase(vendhub_tasks_created_total[24h])) < 0.7
        for: 1h
        labels:
          severity: warning
          component: business
        annotations:
          summary: "Low task completion rate"
          description: "Task completion rate is {{ $value | humanizePercentage }} - below 70% threshold"

      - alert: HighMachineOfflineRate
        expr: vendhub_machines_offline{reason="total"} / (vendhub_machines_active{status="active"} + vendhub_machines_offline{reason="total"}) > 0.1
        for: 30m
        labels:
          severity: warning
          component: business
        annotations:
          summary: "High machine offline rate"
          description: "{{ $value | humanizePercentage }} of machines are offline"

      - alert: UrgentTasksNotCompleted
        expr: increase(vendhub_tasks_created_total{priority="urgent"}[4h]) - increase(vendhub_tasks_completed_total{priority="urgent", status="completed"}[4h]) > 5
        for: 1h
        labels:
          severity: critical
          component: business
        annotations:
          summary: "Urgent tasks pending"
          description: "{{ $value }} urgent tasks have not been completed in 4 hours"

      - alert: InventoryMovementAnomaly
        expr: rate(vendhub_inventory_movements_total[1h]) == 0 and hour() >= 9 and hour() <= 18
        for: 4h
        labels:
          severity: warning
          component: business
        annotations:
          summary: "No inventory movements"
          description: "No inventory movements recorded in the last 4 hours during business hours"

      - alert: TaskProcessingDelayed
        expr: histogram_quantile(0.95, rate(vendhub_task_duration_seconds_bucket[1h])) > 7200
        for: 1h
        labels:
          severity: warning
          component: business
        annotations:
          summary: "Task processing delayed"
          description: "95th percentile task duration is {{ $value | humanizeDuration }} - above 2 hour threshold"

  # ==========================================================================
  # SLA/SLO Alerts
  # ==========================================================================
  - name: sla_alerts
    interval: 60s
    rules:
      - alert: SLOViolation_Availability
        expr: (1 - avg_over_time(up{job=~"vendhub-backend.*"}[1h])) > 0.01
        for: 5m
        labels:
          severity: critical
          component: slo
        annotations:
          summary: "SLO Violation: Availability below 99%"
          description: "System availability is {{ $value | humanizePercentage }} below target"

      - alert: SLOViolation_Latency
        expr: histogram_quantile(0.99, sum(rate(vendhub_http_request_duration_seconds_bucket[1h])) by (le)) > 2
        for: 10m
        labels:
          severity: warning
          component: slo
        annotations:
          summary: "SLO Violation: P99 latency above 2s"
          description: "P99 latency is {{ $value }}s - above 2s SLO target"

      - alert: SLOViolation_ErrorBudget
        expr: sum(increase(vendhub_http_request_errors_total[24h])) / sum(increase(vendhub_http_requests_total[24h])) > 0.01
        for: 30m
        labels:
          severity: warning
          component: slo
        annotations:
          summary: "SLO Violation: Error budget consumption high"
          description: "Daily error rate is {{ $value | humanizePercentage }} - consuming error budget"
